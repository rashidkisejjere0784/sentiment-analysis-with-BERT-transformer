# sentiment-analysis-with-BERT-transformer

BERT stands for Bidirectional Encoder Representations from Transformers. 
BERT is a bidirectional transformer pre-trained using a combination
of masked language modeling objective and next sentence prediction on a large corpus comprising the
Toronto Book Corpus and Wikipedia. It is designed to pre-train deep bidirectional representations 
from unlabeled text by jointly conditioning on both left and right context in all layers. 
As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer
to create state-of-the-art models for a wide range of tasks, such as question answering and language 
inference, without substantial task-specific architecture modifications1. BERT is 
efficient at predicting masked tokens and at NLU in general, but is not optimal
for text generation1.
